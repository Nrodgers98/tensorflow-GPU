{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1587aa08",
   "metadata": {},
   "source": [
    "# Lesson 28: TensorFlow/Keras classification activity\n",
    "\n",
    "## Notebook set up\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39b9bf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import itertools\n",
    "\n",
    "from pathlib import Path\n",
    "from shutil import rmtree\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# Third party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6707ac",
   "metadata": {},
   "source": [
    "### GPU configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8982d6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "Memory growth enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1769493440.041840    2247 service.cc:145] XLA service 0x5880ced37a20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1769493440.041884    2247 service.cc:153]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "I0000 00:00:1769493440.077200    2247 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "# Configure GPU settings\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # Use only GPU 0\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Enable memory growth\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "        print(f'Using GPU: {gpus[0]}')\n",
    "        print('Memory growth enabled')\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Trigger one-time XLA compilation message so we don't have to look at it later\n",
    "@tf.function(jit_compile=True)\n",
    "def simple_xla_op():\n",
    "    return tf.add(tf.constant(1.0), tf.constant(2.0))\n",
    "\n",
    "_ = simple_xla_op()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ba26d9",
   "metadata": {},
   "source": [
    "### Global TensorFlow training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73b5ddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove and recreate logs directory to clear any old TensorBoard logs\n",
    "log_dir = ('../logs')\n",
    "rmtree(log_dir, ignore_errors=True)\n",
    "Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set verbosity for wherever it's accepted\n",
    "verbose = 0\n",
    "\n",
    "# Fix TensorFlow random state for reproducibility\n",
    "tf.random.set_seed(315)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f5eab8",
   "metadata": {},
   "source": [
    "## 1. Data preparation\n",
    "\n",
    "### 1.1. Load occupancy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e94bf1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Light</th>\n",
       "      <th>CO2</th>\n",
       "      <th>HumidityRatio</th>\n",
       "      <th>Occupancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-02-04 17:51:00</td>\n",
       "      <td>23.18</td>\n",
       "      <td>27.2720</td>\n",
       "      <td>426.0</td>\n",
       "      <td>721.25</td>\n",
       "      <td>0.004793</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-02-04 17:51:59</td>\n",
       "      <td>23.15</td>\n",
       "      <td>27.2675</td>\n",
       "      <td>429.5</td>\n",
       "      <td>714.00</td>\n",
       "      <td>0.004783</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-02-04 17:53:00</td>\n",
       "      <td>23.15</td>\n",
       "      <td>27.2450</td>\n",
       "      <td>426.0</td>\n",
       "      <td>713.50</td>\n",
       "      <td>0.004779</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-02-04 17:54:00</td>\n",
       "      <td>23.15</td>\n",
       "      <td>27.2000</td>\n",
       "      <td>426.0</td>\n",
       "      <td>708.25</td>\n",
       "      <td>0.004772</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-04 17:55:00</td>\n",
       "      <td>23.10</td>\n",
       "      <td>27.2000</td>\n",
       "      <td>426.0</td>\n",
       "      <td>704.50</td>\n",
       "      <td>0.004757</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date  Temperature  Humidity  Light     CO2  HumidityRatio  \\\n",
       "0  2015-02-04 17:51:00        23.18   27.2720  426.0  721.25       0.004793   \n",
       "1  2015-02-04 17:51:59        23.15   27.2675  429.5  714.00       0.004783   \n",
       "2  2015-02-04 17:53:00        23.15   27.2450  426.0  713.50       0.004779   \n",
       "3  2015-02-04 17:54:00        23.15   27.2000  426.0  708.25       0.004772   \n",
       "4  2015-02-04 17:55:00        23.10   27.2000  426.0  704.50       0.004757   \n",
       "\n",
       "   Occupancy  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occupancy_df = pd.read_csv('https://gperdrizet.github.io/FSA_devops/assets/data/unit4/occupancy_data.csv')\n",
    "occupancy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0561763f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20560 entries, 0 to 20559\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   date           20560 non-null  object \n",
      " 1   Temperature    20560 non-null  float64\n",
      " 2   Humidity       20560 non-null  float64\n",
      " 3   Light          20560 non-null  float64\n",
      " 4   CO2            20560 non-null  float64\n",
      " 5   HumidityRatio  20560 non-null  float64\n",
      " 6   Occupancy      20560 non-null  int64  \n",
      "dtypes: float64(5), int64(1), object(1)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "occupancy_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5716bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'Occupancy'\n",
    "features = ['Temperature','Humidity','Light','CO2','HumidityRatio']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdce7333",
   "metadata": {},
   "source": [
    "### 1.2. Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43267e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df, testing_df = train_test_split(occupancy_df, random_state=315)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea0e509",
   "metadata": {},
   "source": [
    "### 1.3. Standard scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63115680",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scaler = StandardScaler()\n",
    "feature_scaler.fit(training_df[features])\n",
    "\n",
    "training_df[features] = feature_scaler.transform(training_df[features])\n",
    "testing_df[features] = feature_scaler.transform(testing_df[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bccff99",
   "metadata": {},
   "source": [
    "## 2. Logistic regression baseline\n",
    "\n",
    "### 2.1. Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8e85966",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = LogisticRegression(n_jobs=-1, random_state=315)\n",
    "fit_result = logistic_model.fit(training_df[features], training_df[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d5c9c2",
   "metadata": {},
   "source": [
    "### 2.2. Test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9eea129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression accuracy on test set: 0.9907\n"
     ]
    }
   ],
   "source": [
    "logistic_predictions = logistic_model.predict(testing_df[features])\n",
    "logistic_accuracy = accuracy_score(testing_df[label], logistic_predictions)\n",
    "print(f'Logistic regression accuracy on test set: {logistic_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f45893",
   "metadata": {},
   "source": [
    "## 3. DNN model\n",
    "\n",
    "### 3.1. Compute class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4d0bdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 0.6505231184610193, 1: 2.1608744394618835}\n"
     ]
    }
   ],
   "source": [
    "# Compute class weights to handle class imbalance\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(training_df[label]),\n",
    "    y=training_df[label]\n",
    ")\n",
    "\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "print(f'Class weights: {class_weight_dict}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb98d209",
   "metadata": {},
   "source": [
    "### 3.2. Callback constructor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef3c4b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_callbacks(\n",
    "    learning_rate: float,\n",
    "    batch_size: int,\n",
    "    patience: int = 50,\n",
    ") -> Tuple[keras.callbacks.EarlyStopping, keras.callbacks.TensorBoard]:\n",
    "    '''Create EarlyStopping and TensorBoard callbacks.'''\n",
    "\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=patience,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    run_log_dir = f'{log_dir}/lr_{learning_rate}_bs_{batch_size}'\n",
    "\n",
    "    Path(run_log_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    tensorboard_callback = keras.callbacks.TensorBoard(\n",
    "        log_dir=run_log_dir,\n",
    "        histogram_freq=1,\n",
    "        write_graph=False\n",
    "    )\n",
    "\n",
    "    return early_stopping, tensorboard_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2260c98f",
   "metadata": {},
   "source": [
    "### 3.3. Model constructor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7026c659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(learning_rate: float=0.01) -> keras.Sequential:\n",
    "    '''Builds and compiles the Keras Sequential model.'''\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(5,)),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aa4af3",
   "metadata": {},
   "source": [
    "### 3.4. Model training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5178fc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: keras.Sequential,\n",
    "    epochs: int=5000,\n",
    "    batch_size: int=128,\n",
    "    validation_split: Optional[float]=0.2,\n",
    "    callbacks: Tuple[keras.callbacks.EarlyStopping, keras.callbacks.TensorBoard]=None\n",
    ") -> Tuple[keras.Sequential, keras.callbacks.History]:\n",
    "    '''Trains the Keras Sequential model. Returns the training history.'''\n",
    "\n",
    "    history = model.fit(\n",
    "        training_df[features],\n",
    "        training_df[label],\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=validation_split,\n",
    "        class_weight=class_weight_dict,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce92b2b",
   "metadata": {},
   "source": [
    "### 3.5. Optimize batch size and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e31840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 16, learning rate: 0.01...\t"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_sizes = [16, 64, 256, None]\n",
    "learning_rates = [0.01, 0.005, 0.0025]\n",
    "\n",
    "for batch_size, learning_rate in itertools.product(batch_sizes, learning_rates):\n",
    "\n",
    "    print(f'Batch size: {batch_size}, learning rate: {learning_rate}...', end='\\t')\n",
    "\n",
    "    # Set-up callback for this run\n",
    "    callbacks = make_callbacks(learning_rate=learning_rate, batch_size=batch_size)\n",
    "\n",
    "    # Build and train the model\n",
    "    model = build_model(learning_rate=learning_rate)\n",
    "    model, history = train_model(model, batch_size=batch_size, callbacks=callbacks)\n",
    "\n",
    "    print(f\"final val loss: {history.history['val_loss'][-1]:.4f}, \", end=' ') \n",
    "    print(f\"final val accuracy: {history.history['val_accuracy'][-1]*100:.2f}%\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcb39fb",
   "metadata": {},
   "source": [
    "Based on the final loss & accuracy and the TensorBoard learning curves the best settings are:\n",
    "\n",
    "- Batch size: 256\n",
    "- Learning rate: 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23813338",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ac5759",
   "metadata": {},
   "source": [
    "### 3.6. Re-train with optimized hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff678452",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = make_callbacks(learning_rate=learning_rate, batch_size=batch_size)\n",
    "model = build_model(learning_rate=learning_rate)\n",
    "model, history = train_model(model, batch_size=batch_size, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74f3b7e",
   "metadata": {},
   "source": [
    "### 3.7. Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f48c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(9, 4))\n",
    "\n",
    "axes[0].set_title('Optimized DNN: loss')\n",
    "axes[0].plot(history.history['loss'], label='Training')\n",
    "axes[0].plot(history.history['val_loss'], label='Validation')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (binary crossentropy)')\n",
    "axes[0].legend(loc='best')\n",
    "\n",
    "axes[1].set_title('Optimized DNN: accuracy')\n",
    "axes[1].plot(history.history['accuracy'], label='Training')\n",
    "axes[1].plot(history.history['val_accuracy'], label='Validation')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend(loc='best')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f32761",
   "metadata": {},
   "source": [
    "### 3.8. Test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2e6fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_probs = model.predict(testing_df[features], verbose=0).flatten()\n",
    "sequential_predictions = (sequential_probs >= 0.5).astype(int)\n",
    "sequential_accuracy = accuracy_score(testing_df[label], sequential_predictions)\n",
    "\n",
    "print(f'Optimized DNN model accuracy on test set: {sequential_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8be77f",
   "metadata": {},
   "source": [
    "### 3.9. Performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97451643",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(4, 4))\n",
    "\n",
    "ax.set_title(f'Optimized DNN test set performance\\n(accuracy: {sequential_accuracy:.2%})')\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_predictions(\n",
    "    testing_df[label],\n",
    "    sequential_predictions,\n",
    "    normalize='true',\n",
    "    ax=ax,\n",
    "    colorbar=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717d75c2",
   "metadata": {},
   "source": [
    "## 4. Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905307f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Logistic regression accuracy on test set: {logistic_accuracy:.4f}')\n",
    "print(f'Optimized DNN model accuracy on test set: {sequential_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cc4e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare confusion matrices for logistic regression and DNN\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(7, 4.5))\n",
    "\n",
    "fig.suptitle('Test set performance comparison')\n",
    "fig.supxlabel('Predicted label')\n",
    "fig.supylabel('True label')\n",
    "\n",
    "# Confusion matrix for logistic regression\n",
    "axes[0].set_title(f'Logistic regression\\n(accuracy: {logistic_accuracy:.2%})')\n",
    "\n",
    "disp_logistic = ConfusionMatrixDisplay.from_predictions(\n",
    "    testing_df[label],\n",
    "    logistic_predictions,\n",
    "    normalize='true',\n",
    "    ax=axes[0],\n",
    "    colorbar=False\n",
    ")\n",
    "\n",
    "axes[0].set_xlabel('')\n",
    "axes[0].set_ylabel('')\n",
    "\n",
    "# Confusion matrix for DNN model\n",
    "axes[1].set_title(f'DNN\\n(accuracy: {sequential_accuracy:.2%})')\n",
    "\n",
    "disp_sequential = ConfusionMatrixDisplay.from_predictions(\n",
    "    testing_df[label],\n",
    "    sequential_predictions,\n",
    "    normalize='true',\n",
    "    ax=axes[1],\n",
    "    colorbar=False\n",
    ")\n",
    "\n",
    "axes[1].set_xlabel('')\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfc7d39",
   "metadata": {},
   "source": [
    "## 5. Final training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d1ca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine training and testing datasets for final training\n",
    "combined_df = pd.concat([training_df, testing_df], ignore_index=True)\n",
    "\n",
    "# Build final model with optimized hyperparameters\n",
    "final_model = build_model(learning_rate=learning_rate)\n",
    "\n",
    "# Train on combined dataset (no validation split for final run)\n",
    "final_history = final_model.fit(\n",
    "    combined_df[features],\n",
    "    combined_df[label],\n",
    "    epochs=200,\n",
    "    batch_size=batch_size,\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=verbose\n",
    ")\n",
    "\n",
    "print(f\"Final training accuracy: {final_history.history['accuracy'][-1]*100:.2f}%\")\n",
    "print(f\"Final training loss: {final_history.history['loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa40c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "Path('../models').mkdir(parents=True, exist_ok=True)\n",
    "final_model.save('../models/optimized_occupancy_dnn_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
